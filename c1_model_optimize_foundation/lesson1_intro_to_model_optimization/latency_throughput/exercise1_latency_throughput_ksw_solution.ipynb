{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2e3b77",
   "metadata": {},
   "source": [
    "# Exercise 1 â€” KSW Solution\n",
    "\n",
    "Build on the **demo** by exploring: (A) generation length, (B) numerical precision, and (C) KV cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760f2f5",
   "metadata": {},
   "source": [
    "## 0) Environment guard\n",
    "\n",
    "#### Built on top of the Demo\n",
    "Avoid NumPy 2.x + TensorFlow wheel conflicts by disabling optional TF/Flax imports in `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5335a",
   "metadata": {},
   "source": [
    "## 1) Imports & globals\n",
    "\n",
    "#### Built on top of the Demo\n",
    "If the preferred model isn't available, we fall back to a tiny model to keep the notebook runnable everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, Any, List\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device={DEVICE}; NumPy={np.__version__}; Torch={torch.__version__}\")\n",
    "\n",
    "PREFERRED_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "DEFAULT_PROMPT = (\n",
    "        \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural \"\n",
    "        \"intelligence displayed by humans and animals. Leading AI textbooks define the field as the study \"\n",
    "        \"of intelligent agents: any system that perceives its environment and takes actions that maximize \"\n",
    "        \"its chance of achieving its goals. Colloquially, the term \\\"artificial intelligence\\\" is often \"\n",
    "        \"used to describe machines that mimic cognitive functions that humans associate with the human mind, \"\n",
    "        \"such as learning and problem-solving.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447515ab",
   "metadata": {},
   "source": [
    "## 2) Helpers \n",
    "\n",
    "#### Built on top of the Demo\n",
    "**Todo #1: Resolve Requested Numeric Precision** \\\n",
    "In the _resolve_dtype function\n",
    "- write the logic to convert a user-provided precision string (\"float16\" or \"float32\") into the corresponding torch.dtype.\n",
    "- if the user provided string is neither float16 nor float32 , raise ValueError with a clear message.\n",
    "- Your implementation must include a CPU guard: if the device is CPU and the user requests float16, the function should fall back to torch.float32 and return a descriptive label like \"float32 (forced on CPU)\". This is because most CPUs donâ€™t support fp16 efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb97ec0-bd7e-45e7-ae20-357059dc0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resolve_dtype(dtype_str: str, device: torch.device):\n",
    "    \"\"\"\n",
    "      1) Normalize/validate `dtype_str` (accept 'float16' or 'float32'; case-insensitive is fine).\n",
    "         - If unsupported, raise ValueError with a clear message.\n",
    "\n",
    "      2) CPU guard:\n",
    "         - If device.type == 'cpu' and requested dtype is float16, fall back to torch.float32\n",
    "           (most CPUs donâ€™t support fp16 efficiently).\n",
    "         - Return both: (torch.float32, \"float32 (forced on CPU)\").\n",
    "\n",
    "      3) Otherwise map the string to the torch dtype:\n",
    "         - 'float16' -> torch.float16\n",
    "         - 'float32' -> torch.float32\n",
    "\n",
    "      4) Return a tuple: (resolved_torch_dtype, label_string), where label_string is\n",
    "         a human-readable descriptor like 'float16' or 'float32 (forced on CPU)'.\n",
    "    \"\"\"\n",
    "    if not isinstance(dtype_str, str):\n",
    "        raise ValueError(f\"dtype_str must be a string, got {type(dtype_str).__name__}\")\n",
    "\n",
    "\n",
    "    # Normalize\n",
    "    normalized_dtype = dtype_str.strip().lower()\n",
    "\n",
    "    # Check and Return\n",
    "    if device.type == 'cpu' and dtype == 'float16':\n",
    "        normalized_dtype = 'float32 (forced on CPU)'\n",
    "        return torch.float32, normalized_dtype\n",
    "\n",
    "    if dtype == 'float16':\n",
    "        return torch.float16, normalized_dtype\n",
    "    elif dtype == 'float32':\n",
    "        return torch.float32, normalized_dtype\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype '{dtype_str}'. Supported values are: 'float16' or 'float32'.\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26dcbc-eeae-4b0a-9359-24aada6e42a5",
   "metadata": {},
   "source": [
    "## TODO 2: Load the Model and Tokenizer: \n",
    "Implement the load_model_and_tokenizer function. \n",
    "- This function should first call your _resolve_dtype helper to get the correct data type.\n",
    "- Then, use AutoTokenizer.from_pretrained and AutoModelForCausalLM.from_pretrained to load the assets.\n",
    "- Ensure the model is moved to the correct device, set to evaluation mode with .eval(), and that the tokenizer's pad_token is set if necessary to avoid warnings.def load_model_and_tokenizer(model_name: str, dtype_str: str = \"float16\"):\n",
    "\n",
    "## Code Notes\n",
    "\n",
    "1ï¸âƒ£ low_cpu_mem_usage=True\n",
    "\n",
    "**What it means**\n",
    "```\n",
    "Load the model without ever holding a full copy in CPU RAM.\n",
    "```\n",
    "Normally, from_pretrained():\n",
    "- Downloads weights to disk\n",
    "- Loads them fully into CPU memory\n",
    "- Copies them to GPU\n",
    "\n",
    "For big models, step (2) can blow up your RAM.\n",
    "\n",
    "**With low_cpu_mem_usage=True:**\n",
    "- Weights are streamed\n",
    "- Layers are materialized one by one\n",
    "- Much lower peak RAM usage\n",
    "\n",
    "**When to use it**\n",
    "- Large models\n",
    "- Limited system RAM\n",
    "- Servers / containers / CI\n",
    "\n",
    "**Tradeoffs**\n",
    "- Slightly slower load time\n",
    "- Requires Hugging Face ðŸ¤— accelerate under the hood\n",
    "\n",
    "2ï¸âƒ£ trust_remote_code=False (default, safe)\n",
    "Meaning\n",
    "```\n",
    "â€œIf this repo requires custom Python code, do NOT run it.â€\n",
    "```\n",
    "\n",
    "When you do:\n",
    "```\n",
    "AutoModelForCausalLM.from_pretrained(\"some-model\")\n",
    "```\n",
    "\n",
    "Transformers normally:\n",
    "- Downloads weights (.bin, .safetensors)\n",
    "- Instantiates a built-in model class (e.g. LlamaForCausalLM)\n",
    "- Loads the weights into that class\n",
    "ðŸ‘‰ No custom code runs.\n",
    "\n",
    "**What changes when a repo has â€œremote codeâ€**\n",
    "\n",
    "Some model repos include Python files like:\n",
    "- modeling_my_model.py\n",
    "- configuration_my_model.py\n",
    "- generation_utils.py\n",
    "\n",
    "\n",
    "These files can:\n",
    "- Define new model classes\n",
    "- Override forward()\n",
    "- Change attention, KV cache, generation, etc.\n",
    "- Run arbitrary Python at import time\n",
    "\n",
    "Thatâ€™s what Hugging Face calls remote code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a32ac59-2180-4420-81e5-07696ffd2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_names: list[str], dtype_str: str = \"float16\"):\n",
    "    \"\"\"\n",
    "      1) Resolve the requested precision:\n",
    "         - Call `_resolve_dtype(dtype_str, device)` to get `(torch_dtype, dtype_label)`.\n",
    "         - Ensure CPU requests for 'float16' are coerced to float32.\n",
    "\n",
    "      2) Build a candidate list of model ids:\n",
    "         - Start with `[model_name]`.\n",
    "         - (Optional) Append `FALLBACK_MODELS` so the function still works if the preferred model\n",
    "           isnâ€™t available in the environment.\n",
    "\n",
    "      3) Try candidates in order:\n",
    "         for name in candidates:\n",
    "           try:\n",
    "             - tokenizer = AutoTokenizer.from_pretrained(name, use_fast=True)\n",
    "             - model = AutoModelForCausalLM.from_pretrained(name, torch_dtype=torch_dtype)\n",
    "             - Move model to `device` and call `model.eval()`\n",
    "             - If `tokenizer.pad_token_id` is None and `tokenizer.eos_token_id` exists,\n",
    "               set `tokenizer.pad_token = tokenizer.eos_token` to avoid padding warnings.\n",
    "             - RETURN `(tokenizer, model, name, dtype_label)`\n",
    "           except Exception as e:\n",
    "             - Collect the error and continue to the next candidate.\n",
    "\n",
    "      4) If all candidates fail:\n",
    "         - Raise `RuntimeError` with a concise message and include the first/last error\n",
    "           to aid debugging.\n",
    "\n",
    "    Notes:\n",
    "      - You may pass `low_cpu_mem_usage=True` or `device_map=\"auto\"` (if appropriate for your runtime).\n",
    "      - Keep `trust_remote_code=False` unless you explicitly need custom modeling code.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Resolve model type\n",
    "    torch_dtype, normalized_dtype = _resolve_dtype(dtype_str, DEVICE)\n",
    "\n",
    "    errors = []\n",
    "    # Try model candidates in order\n",
    "    for mname in model_names:\n",
    "        try: \n",
    "            # Get Tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(mname, use_fast=True)\n",
    "            # Get Model\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                mname,\n",
    "                torch_dtype=torch_dtype,\n",
    "                trust_remote_code=False,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            \n",
    "            # Deploy mode to device\n",
    "            model.to(DEVICE)\n",
    "            \n",
    "            # Put model in eval mode\n",
    "            model.eval()\n",
    "\n",
    "            # Set `tokenizer.pad_token = tokenizer.eos_token` to avoid padding warnings.\n",
    "            if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                \n",
    "            return tokenizer, model, mname, normalized_dtype\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append((mname, e))\n",
    "\n",
    "    first_err = errors[0][1] if errors else \"unknown\"\n",
    "    last_err = errors[-1][1] if errors else \"unknown\"\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"Failed to load any model from {model_names}. \"\n",
    "        f\"First error: {first_err}. Last error: {last_err}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65be4ed-6a86-4c3b-8749-614cac61365e",
   "metadata": {},
   "source": [
    "### Time Generate\n",
    "Code Notes\n",
    "\n",
    "#### 1)     inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "- tokenizer returns an input dictionary of tensors\n",
    "- to.device() deploys the input to the device (just like how model is deployed to device)\n",
    "- both the model and inputs should be deployed to the same device otherwise it will error out\n",
    "\n",
    "#### 2) torch.no_grad(): Warmup\n",
    "```\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmup):\n",
    "            output_ids = model_generate(input_ids, max_new_tokens = 8, use_cache = use_cache)\n",
    "```\n",
    "- in this you are running the inference loop , with reduced number of tokens = 8. You are running this multiple times\n",
    "- **ðŸŽ¯ why torch.no_grad():** disables gradient checking. You are running inference, so no need for gradient checking unnecessary\n",
    "   - Uses less memory\n",
    "   - Runs faster\n",
    "   - Prevents accidental training behavior\n",
    "   - even if you forget to the torch.no_grad(). It will still run. But it will be slower and use more memory\n",
    "- **ðŸŽ¯ why warmup :**\n",
    "  - CUDA context setup: this itself takes 100-500ms\n",
    "  - GPU kernel initialization: Many GPU operations select optimized kernels.Tune algorithms for your tensor shapes. Cache best execution strategy\n",
    "  - Memory allocation: PyTorch builds its memory allocator pool. Allocates large blocks for future reuse\n",
    "  - JIT kernel compilation\n",
    "  - Cache building\n",
    "- **ðŸŽ¯ why warmup multiple times :** usually 1 warmup is enough, but multiple warmups help based on the context\n",
    "  - GPU frequency scaling: GPUs start in a lower power state. After a few runs, they boost to full clocks.\n",
    "  - Memory pool stabilization: First few iterations may still trigger new allocations.\n",
    "  - Caching behavior: Some internal optimizations settle after a few runs.\n",
    "  - Variance smoothing: You want stable latency before measuring.\n",
    "- **ðŸŽ¯ How many warmups do we need:** based on context\n",
    "  - If you're benchmarking latency seriously: Use 5â€“10 warmup iterations.\n",
    "  - If you're just running inference: You donâ€™t need warmup at all.\n",
    "  - If you're building a production API: One warmup call at startup is enough.Production cares about user latency, not benchmark purity\n",
    "- **ðŸŽ¯ If use_cache=True, the first generation:**\n",
    "   - KV Cache allocation: Allocates KV cache buffers. Sets up attention structures\n",
    "   - KV cache stores past attention K and V (keys and values) so the model doesnâ€™t recompute them every token. Only the new tokenâ€™s Q/K/V is computed We donâ€™t cache queries because queries are only used once\n",
    "   - use_cache = True is used during inference only. If you're doing autoregressive text generation:ðŸ‘‰ Always use use_cache=True unless you have a very specific reason not to.\n",
    "  - use_cache NEVER used in TRAINING\n",
    "  - without use_cache in Training, total complexity = O(nÂ³). With use_cache = True, total complexity in training = O(nÂ²)\n",
    "- **ðŸŽ¯ torch.cuda.synchronize() during warmup**\n",
    "   - I included it so that cpu and gpu are synced before the timed inferences begin.\n",
    "   - But generally cuda.synchronize() is not needed during warmup\n",
    "More eloquent answer from chatgpt on why synchronize after warmup is useful\n",
    "   - Python may start queuing multiple generate calls in quick succession\n",
    "   - GPU might still be doing the first warmup operation\n",
    "   - First real inference might still see slightly higher latency because GPU is still initializing some things (from the warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c498bb-8edc-4e70-bf39-fa2dccb5ac38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtime_generate\u001b[39m(\n\u001b[32m      2\u001b[39m     model,\n\u001b[32m      3\u001b[39m     tokenizer,\n\u001b[32m      4\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m      5\u001b[39m     max_new_tokens: \u001b[38;5;28mint\u001b[39m = \u001b[32m50\u001b[39m,\n\u001b[32m      6\u001b[39m     use_cache: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     num_warmup: \u001b[38;5;28mint\u001b[39m = \u001b[32m1\u001b[39m,\n\u001b[32m      8\u001b[39m     num_runs: \u001b[38;5;28mint\u001b[39m = \u001b[32m3\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ) -> \u001b[43mDict\u001b[49m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m     10\u001b[39m     \n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# 1) Tokenize the prompt:\u001b[39;00m\n\u001b[32m     12\u001b[39m     inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# 2) Warmup \u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "def time_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    use_cache: bool = True,\n",
    "    num_warmup: int = 1,\n",
    "    num_runs: int = 3,\n",
    ") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "      1) Tokenize the prompt:\n",
    "         - Create `input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)`.\n",
    "\n",
    "      2) Warm up (no timing):\n",
    "         - Under `torch.no_grad()`, run `model.generate(input_ids, max_new_tokens=8, use_cache=use_cache)`\n",
    "           `num_warmup` times to stabilize kernels/clocks.\n",
    "         - If CUDA, you may `torch.cuda.synchronize()` after warmup.\n",
    "\n",
    "      3) Timed runs:\n",
    "         - Initialize `latencies = []` and `tokens_out = []`.\n",
    "         - Under `torch.no_grad()`, loop `num_runs` times:\n",
    "             * If CUDA, `torch.cuda.synchronize()` before starting the timer.\n",
    "             * Record `t0 = time.perf_counter()`.\n",
    "             * Call `out = model.generate(input_ids, max_new_tokens=max_new_tokens, use_cache=use_cache)`.\n",
    "             * If CUDA, `torch.cuda.synchronize()` to ensure all work has finished.\n",
    "             * Record `t1 = time.perf_counter()` and append `(t1 - t0)` to `latencies`.\n",
    "             * Compute generated token count: `gen = out.shape[-1] - input_ids.shape[-1]`\n",
    "               and append to `tokens_out`.\n",
    "\n",
    "      4) Aggregate metrics:\n",
    "         - `L = average(latencies)` (seconds)\n",
    "         - `TG = average(tokens_out)` (tokens)\n",
    "         - `TPS = TG / L` (tokens/sec), guard divide-by-zero\n",
    "         - `LPT = (L / max(TG, 1e-9)) * 1000.0` (ms/token)\n",
    "\n",
    "      5) Return a dict with keys:\n",
    "         - 'total_latency_s', 'tokens_generated', 'tokens_per_sec', 'avg_latency_per_token_ms'\n",
    "\n",
    "      Notes:\n",
    "        - Ensure `model.eval()` was called beforehand.\n",
    "        - Do all forwards under `torch.no_grad()` to avoid autograd overhead.\n",
    "        - We measure **end-to-end** generation time for the given `max_new_tokens`.\n",
    "    \"\"\"\n",
    "    # 1) Tokenize the prompt:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 2) Warmup \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmup):\n",
    "            output_ids = model_generate(input_ids, max_new_tokens = 8, use_cache = use_cache)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # Time Runs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6de2f0-b0b2-4ca0-b2f5-242259adca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    use_cache: bool = True,\n",
    "    num_warmup: int = 1,\n",
    "    num_runs: int = 3,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "      1) Tokenize the prompt:\n",
    "         - Create `input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)`.\n",
    "\n",
    "      2) Warm up (no timing):\n",
    "         - Under `torch.no_grad()`, run `model.generate(input_ids, max_new_tokens=8, use_cache=use_cache)`\n",
    "           `num_warmup` times to stabilize kernels/clocks.\n",
    "         - If CUDA, you may `torch.cuda.synchronize()` after warmup.\n",
    "\n",
    "      3) Timed runs:\n",
    "         - Initialize `latencies = []` and `tokens_out = []`.\n",
    "         - Under `torch.no_grad()`, loop `num_runs` times:\n",
    "             * If CUDA, `torch.cuda.synchronize()` before starting the timer.\n",
    "             * Record `t0 = time.perf_counter()`.\n",
    "             * Call `out = model.generate(input_ids, max_new_tokens=max_new_tokens, use_cache=use_cache)`.\n",
    "             * If CUDA, `torch.cuda.synchronize()` to ensure all work has finished.\n",
    "             * Record `t1 = time.perf_counter()` and append `(t1 - t0)` to `latencies`.\n",
    "             * Compute generated token count: `gen = out.shape[-1] - input_ids.shape[-1]`\n",
    "               and append to `tokens_out`.\n",
    "\n",
    "      4) Aggregate metrics:\n",
    "         - `L = average(latencies)` (seconds)\n",
    "         - `TG = average(tokens_out)` (tokens)\n",
    "         - `TPS = TG / L` (tokens/sec), guard divide-by-zero\n",
    "         - `LPT = (L / max(TG, 1e-9)) * 1000.0` (ms/token)\n",
    "\n",
    "      5) Return a dict with keys:\n",
    "         - 'total_latency_s', 'tokens_generated', 'tokens_per_sec', 'avg_latency_per_token_ms'\n",
    "\n",
    "      Notes:\n",
    "        - Ensure `model.eval()` was called beforehand.\n",
    "        - Do all forwards under `torch.no_grad()` to avoid autograd overhead.\n",
    "        - We measure **end-to-end** generation time for the given `max_new_tokens`.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmup):\n",
    "            _ = model.generate(input_ids, max_new_tokens=8, use_cache=use_cache)\n",
    "    latencies, tokens_out = [], []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            t0 = time.perf_counter()\n",
    "            out = model.generate(input_ids, max_new_tokens=max_new_tokens, use_cache=use_cache)\n",
    "            t1 = time.perf_counter()\n",
    "            latencies.append(t1 - t0)\n",
    "            tokens_out.append(out.shape[-1] - input_ids.shape[-1])\n",
    "    L = sum(latencies) / max(len(latencies), 1)\n",
    "    TG = sum(tokens_out) / max(len(tokens_out), 1)\n",
    "    TPS = (TG / L) if L > 0 else float('nan')\n",
    "    LPT = (L / max(TG, 1e-9)) * 1000.0\n",
    "    return {'total_latency_s': L, 'tokens_generated': TG, 'tokens_per_sec': TPS, 'avg_latency_per_token_ms': LPT}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb22eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xy(xs: List[float], ys: List[float], xlabel: str, ylabel: str, title: str):\n",
    "    plt.figure()\n",
    "    plt.plot(xs, ys, marker='o')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2b066",
   "metadata": {},
   "source": [
    "## 3) Baseline\n",
    "Run one fixed setup as baseline: `max_new_tokens=50`, `float16`, `use_cache=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013825a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model, MODEL_USED, DTYPE_USED = load_model_and_tokenizer(PREFERRED_MODEL, dtype_str='float16')\n",
    "print(f\"Loaded: {MODEL_USED} | dtype={DTYPE_USED}\")\n",
    "baseline = time_generate(model, tokenizer, DEFAULT_PROMPT, max_new_tokens=50, use_cache=True)\n",
    "assert all(k in baseline for k in ['total_latency_s','tokens_generated','tokens_per_sec','avg_latency_per_token_ms'])\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54275b",
   "metadata": {},
   "source": [
    "## 4) Exercise A â€” Generation Length vs Cost\n",
    "Vary `max_new_tokens` and plot latency/throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_varied_lengths():\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Define a list of generation lengths, e.g. lengths = [10, 50, 100, 250].\n",
    "\n",
    "      2) For each L in lengths:\n",
    "         - Call time_generate(model, tokenizer, DEFAULT_PROMPT, max_new_tokens=L, use_cache=True)\n",
    "         - Collect each result dict in results_len.\n",
    "\n",
    "      3) From results_len compute:\n",
    "         - latencies = [r['total_latency_s'] for r in results_len]\n",
    "         - throughputs = [r['tokens_per_sec'] for r in results_len]\n",
    "\n",
    "      4) Plot two charts using plot_xy (one figure per chart):\n",
    "         - Latency vs Generation Length\n",
    "         - Throughput vs Generation Length\n",
    "         (Use simple line+marker plots with grid; no custom colors.)\n",
    "\n",
    "      5) (Optional) return a dict for downstream use:\n",
    "         return {'lengths': lengths, 'latencies': latencies, 'throughputs': throughputs}\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement run_for_varied_lengths() per the TODO above.\")\n",
    "\n",
    "\n",
    "run_for_varied_lengths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff940e7e",
   "metadata": {},
   "source": [
    "## 5) Exercise B â€” Numerical Precision (float16 vs float32)\n",
    "Reload for each precision with identical settings and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd61102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_varied_precision():\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Define the list of precisions to test, e.g. ['float16', 'float32'].\n",
    "\n",
    "      2) For each precision p in the list:\n",
    "         - Load model + tokenizer using load_model_and_tokenizer(PREFERRED_MODEL, dtype_str=p).\n",
    "         - Run time_generate with:\n",
    "             * DEFAULT_PROMPT\n",
    "             * max_new_tokens=50\n",
    "             * use_cache=True\n",
    "         - Store the result dictionary in compare_precision[p].\n",
    "\n",
    "      3) Return compare_precision, which maps precision â†’ generation performance metrics.\n",
    "\n",
    "      4) (Optional) Plot or print results for easier comparison.\n",
    "         Example metrics: latency, tokens/sec, latency per token.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement run_varied_precision() per the TODO above.\")\n",
    "\n",
    "\n",
    "\n",
    "compare_precision = run_varied_precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787471f",
   "metadata": {},
   "source": [
    "## 6) Exercise C â€” KV Cache On vs Off\n",
    "Use the same model and toggle `use_cache`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_results = {}\n",
    "kv_results['use_cache=True'] = time_generate(model, tokenizer, DEFAULT_PROMPT, max_new_tokens=50, use_cache=True)\n",
    "kv_results['use_cache=False'] = time_generate(model, tokenizer, DEFAULT_PROMPT, max_new_tokens=50, use_cache=False)\n",
    "kv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb5214",
   "metadata": {},
   "source": [
    "## 7) (Optional) Compact summary table\n",
    "Summarize the three ideas in a small table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rows = []\n",
    "rows.append({'label':'baseline (fp16, cache on)','dtype':'float16','use_cache':True,'max_new_tokens':50, **baseline})\n",
    "rows.append({'label':'fp32, cache on','dtype':'float32','use_cache':True,'max_new_tokens':50, **compare_precision['float32']})\n",
    "rows.append({'label':'fp16, cache off','dtype':'float16','use_cache':False,'max_new_tokens':50, **kv_results['use_cache=False']})\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f44ba5-9780-4680-9315-d1058716ccb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
